import torch
import json
import argparse
from transformers import AutoTokenizer, AutoModelForCausalLM
from engine import ResearchEngine

# --- æ ¸å¿ƒé…ç½® ---
MODEL_ID = "meta-llama/Meta-Llama-3-8B-Instruct"

def setup_lmeval_engine(args):
    """
    æ­¤å‡½æ•°æ¼”ç¤ºå¦‚ä½•å°†ä½ çš„ã€Œèƒ½é‡èµ¤å­—æ•°å­¦å½’çº¦ã€å¼•æ“ä½œä¸ºåº•å±‚å†…æ ¸ï¼Œ
    æ‰˜èµ·åºå¤§çš„ lm-eval-harness è¯„æµ‹æ°´ç®¡ã€‚
    """
    print("[*] æ­£åœ¨å‘å®˜æ–¹ lm-eval-harness æ¡†æ¶æ¤å…¥æ–°ç®—æ³•å¼•æ“...")
    
    try:
        from lm_eval import simple_evaluate
        from lm_eval.models.huggingface import HFLM
    except ImportError:
        print("\n[!] è­¦å‘Š: å½“å‰ç¯å¢ƒæœªå®‰è£… lm-eval åº“ï¼")
        print("[!] ä½ ç°åœ¨åªèƒ½çœ‹åˆ°å¯¹æ¥ä»£ç ã€‚æœªæ¥åœ¨æœåŠ¡å™¨ä¸Šè¯·å…ˆæ‰§è¡Œ: `pip install lm-eval`")
        print("[!] ä½œä¸ºæ›¿ä»£ï¼Œå°†è¿è¡Œä¸€ä¸ªè¿·ä½ çš„ Few-shot (ICL) Mock æµ‹è¯•...\n")
        run_icl_mock_demo()
        return

    # 1. æ­£å¸¸åŠ è½½åŸè£…æ¨¡å‹
    print(f"[*] ä»ç¡¬ç›˜åŠ è½½åŸè£…æ¨¡å‹ {MODEL_ID} ...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID, 
        torch_dtype=torch.bfloat16, 
        device_map="auto"
    )
    
    # 2. æŒ‚è½½æˆ‘ä»¬æ–°å†™çš„ ResearchEngine (è¿›è¡Œå†…å­˜å®‰å…¨çš„åˆ†å—é…åˆ†)
    # è¿™å°±æ˜¯ä½ æ›¿æ¢ Baseline è·‘åˆ†çš„å…³é”®ä¸€è¡Œï¼
    engine = ResearchEngine(model)
    # å¯¹äºå¸¸è§„ Few-shot è¯„æµ‹ï¼Œå®ƒæœ¬è´¨ä¸Šä¹Ÿæ˜¯ä¸æ–­é€å…¥ä¸€å †é•¿é•¿çš„ Example æ„æˆçš„ Context
    engine.inject('query', top_k=2)
    print("[*] çŒ´å­è¡¥ä¸ (Monkey Patch) æ³¨å…¥å®Œæˆï¼ŒåŸç”Ÿ Attention ç°å·²è¢«æ›¿æ¢ã€‚")
    
    # 3. å°†è¢«é­”æ”¹è¿‡çš„ model å¡å› lm-eval çš„å¤–å£³é‡Œ
    lm_eval_model = HFLM(pretrained=model, tokenizer=MODEL_ID)
    
    # 4. æ‰§è¡Œæ ‡å‡†è¯„æµ‹ (æ¯”å¦‚ mmlu æˆ–è€… arithmetic)
    print(f"[*] å¼€å§‹æ‰§è¡Œå®˜æ–¹è¯„æµ‹ä»»åŠ¡: {args.tasks} ...")
    results = simple_evaluate(
        model=lm_eval_model,
        tasks=args.tasks.split(","),
        num_fewshot=args.num_fewshot,
        batch_size=args.batch_size
    )
    
    # 5. ä¼˜é›…åœ°æ‰“å°ä½ çš„è·‘åˆ†æˆç»©
    print("\n" + "="*50)
    print(" ğŸš€ lm-eval-harness è¯„æµ‹ç»“æŸ ğŸš€")
    print("="*50)
    print(results["results"])

def run_icl_mock_demo():
    print("[*] (Mock æ¨¡å¼) æ‰§è¡Œ Few-shot In-Context Learning é€»è¾‘æ¨æ¼”...")
    
    example_shot_1 = "Q: 1+1= ? A: 2"
    example_shot_2 = "Q: 2+2= ? A: 4"
    example_shot_3 = "Q: 3+3= ? A: 6"
    
    query = "Q: 4+4= ? A:"
    
    print("å‡è®¾æˆ‘ä»¬å°†ä»¥ä¸Šçš„æ ·ä¾‹ä½œä¸ºç‹¬ç«‹çš„ Context Chunks å–‚ç»™å¼•æ“ã€‚")
    print("ä¼ ç»Ÿ Attention: è¿™ 3 ä¸ªæ ·ä¾‹çš„ KV Cache ä¼šè¢«æ‹¼åœ¨ä¸€èµ·ï¼Œå¹³æ–¹çº§å¤æ‚åº¦ã€‚")
    print("èƒ½é‡èµ¤å­—å¼•æ“:")
    print("  1. '1+1=2' çš„ LSE å±€éƒ¨è‡ªç”±èƒ½è¢«ç®—å‡ºã€‚")
    print("  2. '2+2=4' ç­‰çš„ LSE è¢«å¹¶è¡Œç®—å‡ºã€‚")
    print("  3. æœ€ç»ˆåœ¨ Query æ—¶åˆ»ï¼Œç®—æ³•ä¼šæå–å‡ºä¸€ä¸ªå¥åº·çš„ Target_LSEã€‚")
    print("  4. é‚£äº›å¯¹ 4+4 æ— å…³çš„å™ªéŸ³æ ·ä¾‹ï¼Œä¼šè¢«å…¬å¼ -F.softplus(Delta) è‡ªåŠ¨å‹åˆ¶ã€‚")
    print("  5. OOM-Safe å½’çº¦å®Œæˆè¾“å‡ºï¼")
    print("\n[+] Mock æ¼”ç¤ºå®Œæ¯•ï¼Œä½ å¯ä»¥éšæ—¶é€šè¿‡ `pip install lm-eval` æ¿€æ´»çœŸå®è¯„æµ‹å¼•æ“ã€‚")

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("--tasks", type=str, default="mmlu", help="lm-eval çš„æ ‡å‡†è¯„æµ‹é›†åˆ")
    parser.add_argument("--num_fewshot", type=int, default=5, help="æ‰“æ ‡æ ·æœ¬æ•°")
    parser.add_argument("--batch_size", type=str, default="auto")
    args = parser.parse_args()
    
    setup_lmeval_engine(args)
